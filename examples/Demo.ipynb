{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matrix_pomdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Single Objective Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is based on the paper:\n",
    "\n",
    "Corotis, R. B., Ellis, J. H., & Jiang, M. (2005). Modeling of risk-based inspection, maintenance, and life-cycle cost with partially observable Markov decision processes. Structure and Infrastructure Engineering, 1(1), 75–84. https://doi.org/10.1080/15732470412331289305\n",
    "\n",
    "The bridge condition is enhanced through focused maintenance, while inspections refine the state estimation.\n",
    "\n",
    "**Maintenance Actions**  \n",
    "• P₁: No repair  \n",
    "• P₂: Clean and repaint corroded surfaces  \n",
    "• P₃: Repaint and strengthen girders  \n",
    "• P₄: Extensive repair  \n",
    "\n",
    "**Inspection (Observation) Actions**  \n",
    "• O₁: No inspection  \n",
    "• O₂: Visual inspection  \n",
    "• O₃: Ultrasonic inspection  \n",
    "\n",
    "**Combined Actions**  \n",
    "At each decision point, the agent selects an action that combines maintenance and inspection into a single choice, effectively merging the repair strategy and inspection approach:  \n",
    "\n",
    "• P₁ & O₁: No repair + No inspection  \n",
    "• P₁ & O₂: No repair + Visual inspection  \n",
    "• P₁ & O₃: No repair + Ultrasonic inspection  \n",
    "\n",
    "• P₂ & O₁: Clean and repaint corroded surfaces + No inspection  \n",
    "• P₂ & O₂: Clean and repaint corroded surfaces + Visual inspection  \n",
    "• P₂ & O₃: Clean and repaint corroded surfaces + Ultrasonic inspection  \n",
    "\n",
    "• P₃ & O₁: Repaint and strengthen girders + No inspection  \n",
    "• P₃ & O₂: Repaint and strengthen girders + Visual inspection  \n",
    "• P₃ & O₃: Repaint and strengthen girders + Ultrasonic inspection  \n",
    "\n",
    "• P₄ & O₁: Extensive repair + No inspection  \n",
    "• P₄ & O₂: Extensive repair + Visual inspection  \n",
    "• P₄ & O₃: Extensive repair + Ultrasonic inspection\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script sets up a Matrix POMDP environment with specific transition probabilities, rewards, and observation patterns.\n",
    "# The environment is initialized with a given initial state distribution, transition matrices, rewards, and observation patterns.\n",
    "\n",
    "# Define the initial state distribution\n",
    "p_0 = np.array([0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "\n",
    "# Define the rewards for each state-action pair\n",
    "# The rewards are structured in a way that each row corresponds to a state and each column corresponds to an action.\n",
    "r = np.array([\n",
    "    [    0,    -4,   -18,    -5,    -9,   -23,   -25,   -29,   -43,   -40,   -44,   -58],\n",
    "    [    0,    -4,   -18,    -8,   -12,   -26,   -80,   -84,   -98,  -120,  -124,  -138],\n",
    "    [    0,    -4,   -18,   -15,   -19,   -33,  -100,  -104,  -118,  -550,  -554,  -568],\n",
    "    [ -300,  -304,  -318,  -320,  -324,  -338,  -450,  -454,  -468,  -800,  -804,  -818],\n",
    "    [-2000, -2004, -2018, -2050, -2054, -2068, -2500, -2504, -2518, -4000, -4004, -4018]\n",
    "])\n",
    "\n",
    "# Define the transition matrices for each action\n",
    "# In this case, we have four actions, each represented by a 5x5 matrix.\n",
    "\n",
    "P1 = np.array([\n",
    "    [0.80, 0.13, 0.02, 0.00, 0.05],\n",
    "    [0.00, 0.70, 0.17, 0.05, 0.08],\n",
    "    [0.00, 0.00, 0.75, 0.15, 0.10],\n",
    "    [0.00, 0.00, 0.00, 0.60, 0.40],\n",
    "    [0.00, 0.00, 0.00, 0.00, 1.00]\n",
    "])\n",
    "\n",
    "P2 = np.array([\n",
    "    [0.80, 0.13, 0.02, 0.00, 0.05],\n",
    "    [0.00, 0.80, 0.10, 0.02, 0.08],\n",
    "    [0.00, 0.00, 0.80, 0.10, 0.10],\n",
    "    [0.00, 0.00, 0.00, 0.60, 0.40],\n",
    "    [0.00, 0.00, 0.00, 0.00, 1.00]\n",
    "])\n",
    "\n",
    "P3 = np.array([\n",
    "    [0.80, 0.13, 0.02, 0.00, 0.05],\n",
    "    [0.19, 0.65, 0.08, 0.02, 0.06],\n",
    "    [0.10, 0.20, 0.56, 0.08, 0.06],\n",
    "    [0.00, 0.10, 0.25, 0.55, 0.10],\n",
    "    [0.00, 0.00, 0.00, 0.00, 1.00]\n",
    "])\n",
    "\n",
    "P4 = np.array([\n",
    "    [0.80, 0.13, 0.02, 0.00, 0.05],\n",
    "    [0.80, 0.13, 0.02, 0.00, 0.05],\n",
    "    [0.80, 0.13, 0.02, 0.00, 0.05],\n",
    "    [0.80, 0.13, 0.02, 0.00, 0.05],\n",
    "    [0.80, 0.13, 0.02, 0.00, 0.05]\n",
    "])\n",
    "\n",
    "# Repeat each transition matrix three times to match the number of actions\n",
    "p = np.array([*([P1]*3), *([P2]*3), *([P3]*3), *([P4]*3)])\n",
    "\n",
    "# Define the observation patterns for each state\n",
    "O1 = np.array([\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 0.0, 0.0, 0.0, 0.0]\n",
    "])\n",
    "\n",
    "O2 = np.array([\n",
    "    [0.80, 0.20, 0.00, 0.00, 0.00],\n",
    "    [0.20, 0.60, 0.20, 0.00, 0.00],\n",
    "    [0.05, 0.70, 0.25, 0.00, 0.00],\n",
    "    [0.00, 0.30, 0.70, 0.00, 0.00],\n",
    "    [0.00, 0.00, 1.00, 0.00, 0.00]\n",
    "])\n",
    "\n",
    "O3 = np.array([\n",
    "    [0.90, 0.10, 0.00, 0.00, 0.00],\n",
    "    [0.05, 0.90, 0.05, 0.00, 0.00],\n",
    "    [0.00, 0.05, 0.90, 0.05, 0.00],\n",
    "    [0.00, 0.00, 0.05, 0.90, 0.05],\n",
    "    [0.00, 0.00, 0.00, 0.00, 1.00]\n",
    "])\n",
    "\n",
    "# Repeat each observation pattern four times to match the number of actions\n",
    "o = np.array([*([O1, O2, O3]*4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2, 0.2, 0.2, 0.2, 0.2]), {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Matrix POMDP environment with the defined parameters\n",
    "env = gym.make(\"matrix_pomdp/MatrixPOMDP-v0\", p_0=p_0, p=p, o=o, r=r)\n",
    "env.reset( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16 , 0.166, 0.188, 0.16 , 0.326]),\n",
       " np.float64(-700.0000000000001),\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)  # Example step with action 0 do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.8 , 0.13, 0.02, 0.  , 0.05]),\n",
       " np.float64(-258.5999999999999),\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(9) # Example step with action 9 extensive repair and no inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2, 0.2, 0.2, 0.2, 0.2]), {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a separate evaluation environment with the same parameters for evaluation purposes\n",
    "eval_env = gym.make(\"matrix_pomdp/MatrixPOMDP-v0\", p_0=p_0, p=p, o=o, r=r, true_reward=True)\n",
    "eval_env.reset( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.16 , 0.166, 0.188, 0.16 , 0.326]), np.int64(-300), False, False, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gives the true reward for the last action taken in the evaluation environment\n",
    "eval_env.step(0)  # Example step with action 0 do nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.8 , 0.13, 0.02, 0.  , 0.05]), np.int64(-800), False, False, {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env.step(9)  # Example step with action 9 extensive repair and no inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Multi Objective Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the multi objective enviroment we need to modify our reward matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward matrix with two objectives \n",
    "\n",
    "r1 = np.array([\n",
    "    [    0,    -4,   -18,    -5,    -9,   -23,   -25,   -29,   -43,   -40,   -44,   -58],\n",
    "    [    0,    -4,   -18,    -8,   -12,   -26,   -80,   -84,   -98,  -120,  -124,  -138],\n",
    "    [    0,    -4,   -18,   -15,   -19,   -33,  -100,  -104,  -118,  -550,  -554,  -568],\n",
    "    [ -300,  -304,  -318,  -320,  -324,  -338,  -450,  -454,  -468,  -800,  -804,  -818],\n",
    "    [-2000, -2004, -2018, -2050, -2054, -2068, -2500, -2504, -2518, -4000, -4004, -4018]\n",
    "])\n",
    "\n",
    "# Madeup rewards for the second objective\n",
    "r2 = np.array([\n",
    "    [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12],\n",
    "    [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12],\n",
    "    [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12],\n",
    "    [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12],\n",
    "    [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12]\n",
    "])\n",
    "\n",
    "# Combine the two reward matrices into a single array with two objectives\n",
    "r = np.array([*(r1, r2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2, 0.2, 0.2, 0.2, 0.2]), {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Matrix POMDP environment with the defined parameters\n",
    "env = gym.make(\"matrix_pomdp/MatrixPOMDP-v0\", p_0=p_0, p=p, o=o, r=r, multi_objective=True)\n",
    "env.reset( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6667, 0.1925, 0.0376, 0.0042, 0.099 ]),\n",
       " array([-285.2175,   -7.    ]),\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will give a vector of rewards for the last action taken in the environment\n",
    "env.step(6)  # Example step with action 0 do nothing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matrix-pomdp-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
